name: R ‚Üí BigQuery every 15m (WIF)

on:
  schedule:
    - cron: "*/15 * * * *"  # Every 15 minutes
  workflow_dispatch:

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 25
    concurrency:
      group: vald-q15
      cancel-in-progress: false
    
    permissions:
      id-token: write # needed for keyless auth
      contents: read
    
    env:
      GCP_PROJECT: sac-vald-hub
      BQ_DATASET: analytics
    
    steps:
      - uses: actions/checkout@v4
      
      # Keyless auth to GCP via Workload Identity Federation
      - id: auth
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: projects/884700516106/locations/global/workloadIdentityPools/gha-pool/providers/github
          service_account: gha-bq@sac-vald-hub.iam.gserviceaccount.com
          create_credentials_file: true
          token_format: 'access_token'
          access_token_lifetime: '3600s'
      
      # gcloud + bq CLI for post-run queries
      - name: Setup gcloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: sac-vald-hub
          install_components: bq
      
      # Test authentication before proceeding
      - name: Test GCP Authentication
        run: |
          echo "Testing GCP authentication..."
          gcloud auth list --filter=status:ACTIVE --format="value(account)" || {
            echo "‚ùå gcloud authentication failed"
            exit 1
          }
          
          echo "Testing BigQuery access..."
          bq ls --project_id=sac-vald-hub --max_results=1 || {
            echo "‚ùå BigQuery access failed"
            exit 1
          }
          
          echo "‚úÖ Authentication successful"
          echo "Active service account: $(gcloud auth list --filter=status:ACTIVE --format='value(account)')"
          
          # Verify credentials file exists and is readable
          if [[ -n "$GOOGLE_APPLICATION_CREDENTIALS" ]]; then
            echo "Credentials file: $GOOGLE_APPLICATION_CREDENTIALS"
            if [[ -f "$GOOGLE_APPLICATION_CREDENTIALS" ]]; then
              echo "‚úÖ Credentials file exists and is readable"
              # Show first few characters to verify it's a valid JSON
              head -c 50 "$GOOGLE_APPLICATION_CREDENTIALS" | grep -q "{"
              if [[ $? -eq 0 ]]; then
                echo "‚úÖ Credentials file appears to be valid JSON"
              else
                echo "‚ùå Credentials file doesn't appear to be valid JSON"
                exit 1
              fi
            else
              echo "‚ùå Credentials file not found"
              exit 1
            fi
          else
            echo "‚ùå GOOGLE_APPLICATION_CREDENTIALS not set"
            exit 1
          fi
      
      # R setup (binary packages via RSPM where possible)
      - uses: r-lib/actions/setup-r@v2
        with:
          use-public-rspm: true
      
      # System libs needed by common CRAN packages
      - name: System libs
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            libcurl4-openssl-dev libssl-dev libxml2-dev \
            libfontconfig1-dev libfreetype6-dev libpng-dev \
            libtiff5-dev libjpeg-dev zlib1g-dev \
            libharfbuzz-dev libfribidi-dev
      
      # Fail fast if the script path is wrong
      - name: Verify script exists
        run: |
          test -f .github/scripts/run.R || { 
            echo "‚ùå Missing: .github/scripts/run.R"; 
            ls -la .github/scripts/ || echo "No scripts directory found"
            exit 1; 
          }
          echo "‚úÖ R script found"
      
      # Install R packages with proper error handling
      - name: Install R packages (CRAN)
        run: |
          R -q -e "
          cat('Installing required packages...\n')
          
          packages <- c(
            'DBI','bigrquery',
            'dplyr','tidyr','readr','stringr','purrr','tibble',
            'data.table','hms','lubridate',
            'httr','jsonlite','xml2','curl',
            'valdr',
            'rlang','lifecycle'
          )
          
          # Install packages
          install.packages(packages, repos='https://cloud.r-project.org', dependencies=TRUE)
          
          # Verify all packages can be loaded
          failed_packages <- c()
          for(pkg in packages) {
            if(!require(pkg, character.only = TRUE, quietly = TRUE)) {
              failed_packages <- c(failed_packages, pkg)
            }
          }
          
          if(length(failed_packages) > 0) {
            cat('‚ùå Failed to load packages:', paste(failed_packages, collapse=', '), '\n')
            quit(status = 1)
          }
          
          cat('‚úÖ All packages installed and loaded successfully\n')
          "
      
      # Run your R script with better error handling
      - name: Run R pipeline
        env:
          VALD_CLIENT_ID: ${{ secrets.VALD_CLIENT_ID }}
          VALD_CLIENT_SECRET: ${{ secrets.VALD_CLIENT_SECRET }}
          VALD_TENANT_ID: ${{ secrets.VALD_TENANT_ID }}
          VALD_REGION: use
        run: |
          echo "üöÄ Starting VALD data processing..."
          echo "GCP Project: $GCP_PROJECT"
          echo "BQ Dataset: $BQ_DATASET"
          echo "Credentials file: $GOOGLE_APPLICATION_CREDENTIALS"
          
          # Verify credentials are still valid before running R
          gcloud auth list --filter=status:ACTIVE --format="value(account)" > /dev/null || {
            echo "‚ùå Authentication lost before R execution"
            exit 1
          }
          
          # Run R script with explicit error handling
          if Rscript .github/scripts/run.R; then
            echo "‚úÖ R script completed successfully"
          else
            R_EXIT_CODE=$?
            echo "‚ùå R script failed with exit code $R_EXIT_CODE"
            exit 1
          fi
      
      # Create dataset if it doesn't exist
      - name: Ensure BigQuery dataset exists
        if: always()
        run: |
          echo "Ensuring BigQuery dataset exists..."
          bq mk --dataset --location=US "$GCP_PROJECT:$BQ_DATASET" 2>/dev/null || {
            # Check if it failed because dataset already exists
            if bq ls --project_id="$GCP_PROJECT" | grep -q "$BQ_DATASET"; then
              echo "‚úÖ Dataset already exists"
            else
              echo "‚ùå Failed to create dataset and it doesn't exist"
              exit 1
            fi
          }
      
      # Inspect logs for this run (don't fail build if empty yet)
      - name: Check processing logs
        if: always()
        run: |
          echo "Checking processing logs..."
          bq query --use_legacy_sql=false --location=US --format=pretty \
            "SELECT timestamp, level, message 
             FROM \`$GCP_PROJECT.$BQ_DATASET.vald_processing_log\` 
             WHERE run_id = '${{ github.run_id }}' 
             ORDER BY timestamp DESC 
             LIMIT 20" 2>/dev/null || echo "‚ÑπÔ∏è No logs yet or table doesn't exist."
      
      # Compact summary by level
      - name: Create summary report
        if: always()
        run: |
          echo "Creating processing summary..."
          bq query --use_legacy_sql=false --location=US --format=pretty \
            "SELECT level, COUNT(*) AS message_count 
             FROM \`$GCP_PROJECT.$BQ_DATASET.vald_processing_log\` 
             WHERE run_id = '${{ github.run_id }}' 
             GROUP BY level 
             ORDER BY level" 2>/dev/null || echo "‚ÑπÔ∏è No summary available yet."
