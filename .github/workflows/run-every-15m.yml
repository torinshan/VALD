name: R â†’ BigQuery every 15m (WIF)

on:
  schedule:
    - cron: "*/15 * * * *"  # Fixed cron expression
  workflow_dispatch:

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 25
    concurrency:
      group: vald-q15
      cancel-in-progress: false
    
    permissions:
      id-token: write # needed for keyless auth
      contents: read
    
    env:
      GCP_PROJECT: sac-vald-hub
      BQ_DATASET: vald_data  # Match the script default
    
    steps:
      - uses: actions/checkout@v4
      
      # Keyless auth to GCP via Workload Identity Federation
      - id: auth
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: projects/884700516106/locations/global/workloadIdentityPools/gha-pool/providers/github
          service_account: gha-bq@sac-vald-hub.iam.gserviceaccount.com
          export_environment_variables: true
          create_credentials_file: true
      
      # gcloud + bq CLI for post-run queries
      - name: Setup gcloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: sac-vald-hub
          install_components: bq
      
      # Test authentication
      - name: Test GCP Authentication
        run: |
          echo "Testing GCP authentication..."
          gcloud auth list
          echo "Testing BigQuery access..."
          bq ls --project_id=sac-vald-hub || echo "BigQuery test failed"
          echo "Environment variables:"
          echo "GOOGLE_APPLICATION_CREDENTIALS: $GOOGLE_APPLICATION_CREDENTIALS"
          echo "GCLOUD_PROJECT: $GCLOUD_PROJECT"
      
      # R setup (binary packages via RSPM where possible)
      - uses: r-lib/actions/setup-r@v2
        with:
          use-public-rspm: true
      
      # System libs needed by common CRAN packages
      - name: System libs
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            libcurl4-openssl-dev libssl-dev libxml2-dev \
            libfontconfig1-dev libfreetype6-dev libpng-dev \
            libtiff5-dev libjpeg-dev zlib1g-dev \
            libharfbuzz-dev libfribidi-dev
      
      # Fail fast if the script path is wrong
      - name: Verify script exists
        run: |
          test -f .github/scripts/run.R || { 
            echo "Missing: .github/scripts/run.R"; 
            ls -R; 
            exit 1; 
          }
      
      # Install R packages with proper error handling
      - name: Install R packages (CRAN)
        run: |
          R -q -e "
          packages <- c(
            'DBI','bigrquery',
            'dplyr','tidyr','readr','stringr','purrr','tibble',
            'data.table','hms','lubridate',
            'httr','jsonlite','xml2','curl',
            'valdr',
            'rlang','lifecycle'
          )
          
          install.packages(packages, repos='https://cloud.r-project.org')
          
          # Verify all packages loaded successfully
          for(pkg in packages) {
            if(!require(pkg, character.only = TRUE, quietly = TRUE)) {
              stop(paste('Failed to load package:', pkg))
            }
          }
          
          cat('All packages installed and loaded successfully\n')
          "
      
      # Run your R script with better error handling
      - name: Run R pipeline
        env:
          VALD_CLIENT_ID: ${{ secrets.VALD_CLIENT_ID }}
          VALD_CLIENT_SECRET: ${{ secrets.VALD_CLIENT_SECRET }}
          VALD_TENANT_ID: ${{ secrets.VALD_TENANT_ID }}
          VALD_REGION: use
        run: |
          echo "Starting VALD data processing..."
          echo "GCP Project: $GCP_PROJECT"
          echo "BQ Dataset: $BQ_DATASET"
          
          # Run with explicit error handling
          if ! Rscript .github/scripts/run.R; then
            echo "R script failed with exit code $?"
            exit 1
          fi
          
          echo "R script completed successfully"
      
      # Create dataset if it doesn't exist
      - name: Ensure BigQuery dataset exists
        if: always()
        run: |
          echo "Ensuring BigQuery dataset exists..."
          bq mk --dataset --location=US "$GCP_PROJECT:$BQ_DATASET" || echo "Dataset already exists or creation failed"
      
      # Inspect logs for this run (don't fail build if empty yet)
      - name: Check processing logs
        if: always()
        run: |
          echo "Checking processing logs..."
          bq query --use_legacy_sql=false --location=US --format=pretty \
            "SELECT timestamp, level, message 
             FROM \`$GCP_PROJECT.$BQ_DATASET.vald_processing_log\` 
             WHERE run_id = '${{ github.run_id }}' 
             ORDER BY timestamp DESC 
             LIMIT 20" || echo "No logs yet."
      
      # Compact summary by level
      - name: Create summary report
        if: always()
        run: |
          echo "Creating processing summary..."
          bq query --use_legacy_sql=false --location=US --format=pretty \
            "SELECT level, COUNT(*) AS message_count 
             FROM \`$GCP_PROJECT.$BQ_DATASET.vald_processing_log\` 
             WHERE run_id = '${{ github.run_id }}' 
             GROUP BY level 
             ORDER BY level" || echo "No summary available."
