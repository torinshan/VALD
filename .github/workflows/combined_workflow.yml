name: Workload & Model Pipeline (Every 15 min)

on:
  schedule:
    - cron: "*/15 * * * *"
  workflow_dispatch:
    inputs:
      skip_training:
        description: "Skip model training (ingest only)"
        required: false
        type: boolean
        default: false
      start_date:
        description: "Model training start date (YYYY-MM-DD)"
        required: false
        default: "2025-06-01"
      end_date:
        description: "Model training end date (YYYY-MM-DD)"
        required: false
        default: "2025-12-31"

env:
  GCP_PROJECT: sac-vald-hub
  BQ_DATASET: analytics
  BQ_LOCATION: US

  WORKLOAD_TABLE: workload_daily
  READINESS_TABLE: vald_fd_jumps
  ROSTER_TABLE: roster_mapping
  PREDICTIONS_TABLE: readiness_predictions_byname

  TEAM_NAME: sacstate-football

jobs:
  ingest:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          lfs: true
          fetch-depth: 1

      - name: Auth to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
          create_credentials_file: true
          export_environment_variables: true

      - uses: google-github-actions/setup-gcloud@v2

      - uses: r-lib/actions/setup-r@v2
        with:
          use-public-rspm: true

      - name: Install R dependencies
        uses: r-lib/actions/setup-r-dependencies@v2
        with:
          cache-version: 1
          extra-packages: |
            any::bigrquery
            any::DBI
            any::dplyr
            any::tidyr
            any::readr
            any::readxl
            any::stringr
            any::purrr
            any::tibble
            any::data.table
            any::hms
            any::lubridate
            any::httr
            any::jsonlite
            any::curl
            any::gargle
            any::glue
            any::slider
            any::janitor

      - name: System libs
        run: |
          sudo apt-get update
          sudo apt-get install -y libcurl4-openssl-dev libssl-dev libxml2-dev jq

      - name: Verify workload file exists
        run: |
          test -f ".github/Clean_Activities_GPS.xlsx" || (echo "File not found: .github/Clean_Activities_GPS.xlsx"; exit 1)
          ls -lh ".github/Clean_Activities_GPS.xlsx"

      - name: Ingest workload (local xlsx)
        if: ${{ secrets.GCP_SA_KEY != '' && secrets.GCP_PROJECT != '' }}
        env:
          GCP_PROJECT: ${{ env.GCP_PROJECT }}
          BQ_DATASET:  ${{ env.BQ_DATASET }}
          BQ_LOCATION: ${{ env.BQ_LOCATION }}
          BQ_TABLE:    ${{ env.WORKLOAD_TABLE }}
          BQ_WRITE_MODE: MERGE
          LOCAL_FILE_PATH: ".github/Clean_Activities_GPS.xlsx"
          ONEDRIVE_PUBLIC_URL: ""
        run: Rscript ".github/scripts/workload_ingest.R"

      - name: Check ingest success
        id: ingest_check
        run: |
          echo "âœ… Workload ingest completed successfully"
          echo "status=success" >> "$GITHUB_OUTPUT"

    outputs:
      status: ${{ steps.ingest_check.outputs.status }}

  ingest-roster:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          lfs: true
          fetch-depth: 1

      - name: Auth to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
          create_credentials_file: true
          export_environment_variables: true

      - uses: google-github-actions/setup-gcloud@v2

      - uses: r-lib/actions/setup-r@v2
        with:
          use-public-rspm: true

      - name: Install R dependencies (roster)
        uses: r-lib/actions/setup-r-dependencies@v2
        with:
          cache-version: 1
          extra-packages: |
            any::bigrquery
            any::DBI
            any::dplyr
            any::readxl
            any::stringr
            any::tibble
            any::janitor

      - name: System libs
        run: |
          sudo apt-get update
          sudo apt-get install -y libcurl4-openssl-dev libssl-dev libxml2-dev jq

      - name: Verify roster file exists
        run: |
          test -f ".github/Sac State Roster - Summer 2025.xlsx" || (echo "File not found: .github/Sac State Roster - Summer 2025.xlsx"; exit 1)
          ls -lh ".github/Sac State Roster - Summer 2025.xlsx"

      - name: Ingest roster (local xlsx â†’ BigQuery)
        if: ${{ secrets.GCP_SA_KEY != '' && secrets.GCP_PROJECT != '' }}
        env:
          GCP_PROJECT:        ${{ env.GCP_PROJECT }}
          BQ_DATASET:         ${{ env.BQ_DATASET }}
          BQ_LOCATION:        ${{ env.BQ_LOCATION }}
          ROSTER_TABLE:       ${{ env.ROSTER_TABLE }}
          ROSTER_LOCAL_FILE:  ".github/Sac State Roster - Summer 2025.xlsx"
          ROSTER_SHEET:       "Master"
        run: Rscript ".github/scripts/roster_ingest.R"

      - name: Check roster ingest success
        id: roster_check
        run: |
          echo "âœ… Roster ingest completed successfully"
          echo "status=success" >> "$GITHUB_OUTPUT"

    outputs:
      status: ${{ steps.roster_check.outputs.status }}

  check-readiness:
    runs-on: ubuntu-latest
    needs: [ingest, ingest-roster]
    if: needs.ingest.outputs.status == 'success' && needs['ingest-roster'].outputs.status == 'success'

    steps:
      - uses: actions/checkout@v4

      - name: Auth to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
          create_credentials_file: true
          export_environment_variables: true

      - uses: google-github-actions/setup-gcloud@v2

      - name: Ensure jq + envsubst present
        run: |
          if ! command -v jq >/dev/null 2>&1; then
            sudo apt-get update && sudo apt-get install -y jq
          fi
          if ! command -v envsubst >/dev/null 2>&1; then
            sudo apt-get update && sudo apt-get install -y gettext-base
          fi

      - name: Preflight validation
        id: preflight
        shell: bash
        env:
          GCP_PROJECT:    ${{ env.GCP_PROJECT }}
          BQ_DATASET:     ${{ env.BQ_DATASET }}
          BQ_LOCATION:    ${{ env.BQ_LOCATION }}
          WORKLOAD_TABLE: ${{ env.WORKLOAD_TABLE }}
          READINESS_TABLE: ${{ env.READINESS_TABLE }}
          ROSTER_TABLE:   ${{ env.ROSTER_TABLE }}
        run: |
          set -euo pipefail

          echo "=== Environment Configuration ==="
          echo "  GCP_PROJECT=$GCP_PROJECT"
          echo "  BQ_DATASET=$BQ_DATASET"
          echo "  BQ_LOCATION=$BQ_LOCATION"
          echo "  WORKLOAD_TABLE=$WORKLOAD_TABLE"
          echo "  READINESS_TABLE=$READINESS_TABLE"
          echo "  ROSTER_TABLE=$ROSTER_TABLE"
          echo ""

          : "${GCP_PROJECT:?GCP_PROJECT is required}"
          : "${BQ_DATASET:?BQ_DATASET is required}"
          : "${BQ_LOCATION:?BQ_LOCATION is required}"
          : "${WORKLOAD_TABLE:?WORKLOAD_TABLE is required}"
          : "${READINESS_TABLE:?READINESS_TABLE is required}"
          : "${ROSTER_TABLE:?ROSTER_TABLE is required}"

          echo "Checking dataset existsâ€¦"
          bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT" ls "$BQ_DATASET" >/dev/null

          echo "Checking workload table existsâ€¦"
          bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT" show "${GCP_PROJECT}:${BQ_DATASET}.${WORKLOAD_TABLE}" >/dev/null

          echo "Checking readiness table existsâ€¦"
          bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT" show "${GCP_PROJECT}:${BQ_DATASET}.${READINESS_TABLE}" >/dev/null

          echo "Checking roster table existsâ€¦"
          bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT" show "${GCP_PROJECT}:${BQ_DATASET}.${ROSTER_TABLE}" >/dev/null

          echo "Preflight OK"

      - name: Validate roster mapping table
        shell: bash
        env:
          GCP_PROJECT:  ${{ env.GCP_PROJECT }}
          BQ_DATASET:   ${{ env.BQ_DATASET }}
          BQ_LOCATION:  ${{ env.BQ_LOCATION }}
          ROSTER_TABLE: ${{ env.ROSTER_TABLE }}
        run: |
          set -euo pipefail

          echo "ğŸ” Validating roster mapping table..."

          TABLE_REF="${GCP_PROJECT}.${BQ_DATASET}.${ROSTER_TABLE}"
          ROSTER_COUNT=$(
            bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT" \
              query --use_legacy_sql=false --format=csv \
              "$(printf 'SELECT COUNT(*) FROM `%s`' "$TABLE_REF")" | tail -n1
          )

          if [ "$ROSTER_COUNT" -eq 0 ]; then
            echo "âŒ roster_mapping is empty â€“ need official_id â‡„ vald_name rows"
            exit 1
          fi
          echo "âœ… Roster rows: $ROSTER_COUNT"

          SCHEMA=$(bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT" show \
            --schema --format=prettyjson "${GCP_PROJECT}:${BQ_DATASET}.${ROSTER_TABLE}")

          echo "$SCHEMA" | jq -r '.[].name' | grep -q "^official_id$" || {
            echo "âŒ Column 'official_id' missing in roster_mapping"; echo "$SCHEMA" | jq -r '.[].name'; exit 1; }

          echo "$SCHEMA" | jq -r '.[].name' | grep -q "^vald_name$" || {
            echo "âŒ Column 'vald_name' missing in roster_mapping (case sensitive)"; echo "$SCHEMA" | jq -r '.[].name'; exit 1; }

          echo "âœ… Required columns present (official_id, vald_name)"

      - name: Build and run BigQuery MATCH_QUERY (use backticks for identifiers with spaces)
        shell: bash
        env:
          GCP_PROJECT: ${{ env.GCP_PROJECT }}
          BQ_DATASET: ${{ env.BQ_DATASET }}
          ROSTER_TABLE: ${{ env.ROSTER_TABLE }}
          BQ_LOCATION: ${{ env.BQ_LOCATION }}
        run: |
          set -euo pipefail

          echo "Building MATCH_QUERY (with identifiers)..."

          MATCH_QUERY=$(cat <<SQL
          SELECT
            r.official_id AS official_id,
            r.vald_name    AS vald_name,
            v.full_name    AS vald_full_name
          FROM \`$GCP_PROJECT.$BQ_DATASET.$ROSTER_TABLE\` AS r
          JOIN \`$GCP_PROJECT.$BQ_DATASET.vald_fd_jumps\` AS v
            ON r.vald_name = v.full_name
          WHERE 1=1;
          SQL
          )

          # print the query so it's visible in logs (helps debugging)
          echo "----- MATCH_QUERY -----"
          echo "$MATCH_QUERY"
          echo "-----------------------"

          echo "â–¶ Executing BigQuery query..."
          bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT" \
             query --use_legacy_sql=false --format=json "$MATCH_QUERY"

      - name: Check for matching readiness data (CORRECTED MAPPING)
        id: check
        shell: bash
        env:
          GCP_PROJECT:    ${{ env.GCP_PROJECT }}
          BQ_DATASET:     ${{ env.BQ_DATASET }}
          BQ_LOCATION:    ${{ env.BQ_LOCATION }}
          WORKLOAD_TABLE: ${{ env.WORKLOAD_TABLE }}
          READINESS_TABLE: ${{ env.READINESS_TABLE }}
          ROSTER_TABLE:   ${{ env.ROSTER_TABLE }}
        run: |
          set -euo pipefail
          echo "ğŸ” Checking workload â†” readiness matches (via roster mapping, same DATE)â€¦"
          echo ""
          echo "DATA FLOW:"
          echo "  workload_daily.roster_name (ID) = roster_mapping.official_id"
          echo "  roster_mapping.vald_name (NAME) = vald_fd_jumps.full_name"
          echo ""

          # Match the data connection logic from user's local implementation
          # Key fix: Direct name matching without case normalization
          # Using cat with heredoc instead of read to avoid exit code issues
          MATCH_QUERY=$(cat <<'SQL'
          WITH workload_with_off_id AS (
            SELECT DISTINCT
              roster_name AS official_id,
              DATE(date) AS date
            FROM `${GCP_PROJECT}.${BQ_DATASET}.${WORKLOAD_TABLE}`
            WHERE date >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
              AND roster_name IS NOT NULL
          ),
          vald_fd_jumps_with_off_id AS (
            SELECT
              DATE(v.date) AS date,
              r.official_id,
              v.jump_height_readiness,
              v.epf_readiness,
              v.rsi_readiness
            FROM `${GCP_PROJECT}.${BQ_DATASET}.${READINESS_TABLE}` v
            LEFT JOIN `${GCP_PROJECT}.${BQ_DATASET}.${ROSTER_TABLE}` r
              ON v.full_name = r.vald_name
            WHERE v.date >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
              AND r.official_id IS NOT NULL
          ),
          readiness AS (
            SELECT
              date,
              official_id,
              SAFE_DIVIDE(
                IFNULL(CAST(jump_height_readiness AS FLOAT64), 0) +
                IFNULL(CAST(epf_readiness AS FLOAT64), 0) +
                IFNULL(CAST(rsi_readiness AS FLOAT64), 0),
                NULLIF(
                  IF(jump_height_readiness IS NULL, 0, 1) +
                  IF(epf_readiness IS NULL, 0, 1) +
                  IF(rsi_readiness IS NULL, 0, 1),
                  0
                )
              ) AS readiness
            FROM vald_fd_jumps_with_off_id
          ),
          merged_data AS (
            SELECT
              r.official_id,
              r.date
            FROM readiness r
            JOIN workload_with_off_id w
              ON r.official_id = w.official_id
             AND r.date = w.date
            WHERE r.readiness IS NOT NULL
          )
          SELECT
            COUNT(DISTINCT official_id) AS athletes_with_matches,
            COUNT(*) AS total_matches,
            MIN(date) AS earliest_match,
            MAX(date) AS latest_match
          FROM merged_data
          SQL
          )

          # Expand environment variables
          MATCH_QUERY="$(envsubst <<<"$MATCH_QUERY")"

          echo "SQL QUERY:"
          echo "$MATCH_QUERY"
          echo ""

          # Run the query with FULL error capture
          echo "â–¶ Executing BigQuery query..."
          set +e  # Don't exit on error yet - capture it
          RESULT_JSON=$(
            bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT" \
               query --use_legacy_sql=false --format=json "$MATCH_QUERY" 2>&1
          )
          QUERY_EXIT_CODE=$?
          set -e  # Re-enable exit on error

          if [ $QUERY_EXIT_CODE -ne 0 ]; then
            echo ""
            echo "âŒ BIGQUERY EXECUTION FAILED (Exit code: $QUERY_EXIT_CODE)"
            echo "ERROR OUTPUT:"
            echo "$RESULT_JSON"
            echo ""
            echo "ğŸ” DIAGNOSTIC ANALYSIS:"
            
            if echo "$RESULT_JSON" | grep -qi "not found\|does not exist"; then
              echo "â— Issue: Table/Dataset Not Found"
              echo "   Check: ${GCP_PROJECT}.${BQ_DATASET}.${WORKLOAD_TABLE}"
              echo "   Check: ${GCP_PROJECT}.${BQ_DATASET}.${READINESS_TABLE}"
              echo "   Check: ${GCP_PROJECT}.${BQ_DATASET}.${ROSTER_TABLE}"
            fi
            
            if echo "$RESULT_JSON" | grep -qi "unrecognized\|column"; then
              echo "â— Issue: Column Not Found"
              echo "   Ensure roster_mapping has columns: official_id, vald_name"
            fi
            
            if echo "$RESULT_JSON" | grep -qi "permission\|denied"; then
              echo "â— Issue: Permission Denied"
            fi
            
            echo ""
            echo "has_matches=false" >> "$GITHUB_OUTPUT"
            echo "reason=query_failed" >> "$GITHUB_OUTPUT"
            exit 1
          fi

          echo "âœ… Query executed successfully"
          echo ""

          ATHLETES="$(echo "$RESULT_JSON" | jq -r '.[0].athletes_with_matches // 0')"
          MATCHES="$(echo   "$RESULT_JSON" | jq -r '.[0].total_matches // 0')"
          EARLIEST="$(echo  "$RESULT_JSON" | jq -r '.[0].earliest_match // "none"')"
          LATEST="$(echo    "$RESULT_JSON" | jq -r '.[0].latest_match // "none"')"

          echo "ğŸ“Š QUERY RESULTS:"
          echo "  Athletes with matches: $ATHLETES"
          echo "  Total match records:   $MATCHES"
          echo "  Date range:            $EARLIEST â†’ $LATEST"
          echo ""

          if [ "$MATCHES" -gt 0 ]; then
            echo "âœ… SUCCESS: Found matching readiness data"
            echo "   â†’ Models can be trained for $ATHLETES athletes"
            echo ""
            echo "has_matches=true" >> "$GITHUB_OUTPUT"
            echo "match_count=$MATCHES" >> "$GITHUB_OUTPUT"
            echo "athlete_count=$ATHLETES" >> "$GITHUB_OUTPUT"
          else
            echo "âš ï¸  NO MATCHES FOUND"
            echo ""
            echo "Possible reasons:"
            echo "  1. No workload data in last 7 days"
            echo "  2. No VALD tests in last 7 days"
            echo "  3. Athletes have workload OR VALD, but not BOTH on same dates"
            echo "  4. Name mismatches: roster 'vald_name' â‰  vald_fd_jumps 'full_name'"
            echo ""
            echo "â­ï¸  Training will be skipped (this is normal if no recent data)"
            echo ""
            echo "has_matches=false" >> "$GITHUB_OUTPUT"
            echo "reason=no_readiness_match" >> "$GITHUB_OUTPUT"
          fi

    outputs:
      has_matches:   ${{ steps.check.outputs.has_matches }}
      match_count:   ${{ steps.check.outputs.match_count }}
      athlete_count: ${{ steps.check.outputs.athlete_count }}
      reason:        ${{ steps.check.outputs.reason }}

  train-models:
    runs-on: ubuntu-latest
    needs: [ingest, check-readiness]
    if: |
      needs.ingest.outputs.status == 'success' &&
      needs.check-readiness.outputs.has_matches == 'true' &&
      github.event.inputs.skip_training != 'true'

    steps:
      - uses: actions/checkout@v4

      - name: Auth to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
          create_credentials_file: true
          export_environment_variables: true

      - uses: google-github-actions/setup-gcloud@v2

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install scikit-learn==1.5.1 numpy==1.26.4

      - uses: r-lib/actions/setup-r@v2
        with:
          use-public-rspm: true

      - uses: r-lib/actions/setup-r-dependencies@v2
        with:
          cache-version: 1
          extra-packages: |
            any::bigrquery
            any::DBI
            any::dplyr
            any::tidyr
            any::data.table
            any::lubridate
            any::slider
            any::stringr
            any::gargle
            any::glue
            any::digest
            any::glmnet
            any::bnlearn
            any::reticulate

      - name: System libs
        run: |
          sudo apt-get update
          sudo apt-get install -y libcurl4-openssl-dev libssl-dev libxml2-dev jq

      - name: Verify BigQuery write access
        env:
          GCP_PROJECT: ${{ env.GCP_PROJECT }}
          BQ_DATASET:  ${{ env.BQ_DATASET }}
          BQ_LOCATION: ${{ env.BQ_LOCATION }}
        run: |
          set -euo pipefail
          echo "=== BigQuery Access Validation ==="
          echo "Checking access to dataset ${BQ_DATASET}..."
          
          if ! bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT" ls "${BQ_DATASET}" >/dev/null 2>&1; then
            echo ""
            echo "âŒ Cannot list dataset ${BQ_DATASET}"
            echo ""
            echo "Troubleshooting:"
            echo "  1. Service account needs BigQuery Data Editor permission"
            echo "  2. Or roles/bigquery.dataEditor on the dataset"
            echo "  3. Verify GCP_SA_KEY has BigQuery write access"
            echo ""
            echo "Attempting to show more details:"
            bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT" ls 2>&1 || true
            exit 1
          fi
          echo "âœ… BigQuery dataset accessible"
          
          # Verify we can query the dataset
          if ! bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT" query --use_legacy_sql=false "SELECT 1" >/dev/null 2>&1; then
            echo "âŒ Cannot execute queries in BigQuery"
            exit 1
          fi
          echo "âœ… BigQuery write access verified"

      - name: Run model training
        env:
          GCP_PROJECT: ${{ env.GCP_PROJECT }}
          BQ_DATASET:  ${{ env.BQ_DATASET }}
          BQ_LOCATION: ${{ env.BQ_LOCATION }}
          TEAM_NAME:   ${{ env.TEAM_NAME }}

          WORKLOAD_TABLE:    ${{ env.WORKLOAD_TABLE }}
          READINESS_TABLE:   ${{ env.READINESS_TABLE }}
          ROSTER_TABLE:      ${{ env.ROSTER_TABLE }}
          PREDICTIONS_TABLE: ${{ env.PREDICTIONS_TABLE }}

          START_DATE:        ${{ github.event.inputs.start_date || '2025-06-01' }}
          END_DATE:          ${{ github.event.inputs.end_date   || '2025-12-31' }}
          TRAIN_FRACTION:    '0.80'
          MIN_TRAIN_OBS:     '3'

          HAS_MATCHES:       ${{ needs.check-readiness.outputs.has_matches }}
          MATCH_LOOKBACK_DAYS: '7'
          SKIP_TRAINING:     ${{ github.event.inputs.skip_training || 'false' }}
          SKIP_REGISTRY_STAGES: 'true'  # BigQuery free tier doesn't support DML (INSERT) queries
        run: Rscript ".github/scripts/readiness_models_cloud.R"

      - name: Show today's training summary (best-effort)
        if: success()
        env:
          GCP_PROJECT: ${{ env.GCP_PROJECT }}
          BQ_DATASET:  ${{ env.BQ_DATASET }}
          BQ_LOCATION: ${{ env.BQ_LOCATION }}
        run: |
          set -eo pipefail
          if ! command -v envsubst >/dev/null 2>&1; then
            sudo apt-get update && sudo apt-get install -y gettext-base
          fi

          # Check if model_training_summary table exists before querying
          TABLE_EXISTS=$(bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT" \
            ls -n 1000 "${BQ_DATASET}" 2>/dev/null | grep -c "model_training_summary" || echo "0")

          echo ""
          echo "ğŸ“ˆ Latest Models:"

          if [ "$TABLE_EXISTS" -eq "0" ]; then
            echo "No training results yet (table not created)"
          else
            # Use cat with heredoc instead of read to avoid exit code issues
            Q1=$(cat <<'SQL'
            SELECT
              athlete_id,
              roster_name,
              model_type,
              ROUND(primary_rmse, 3) AS rmse,
              n_train,
              FORMAT_TIMESTAMP('%H:%M', trained_at) AS time
            FROM `${GCP_PROJECT}.${BQ_DATASET}.model_training_summary`
            WHERE DATE(trained_at) = CURRENT_DATE()
            ORDER BY trained_at DESC
            LIMIT 10
            SQL
            )
            Q1="$(envsubst <<<"$Q1")"
            bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT" query --use_legacy_sql=false --max_rows=10 "$Q1" 2>&1 || echo "No training results yet"
          fi

          echo ""
          echo "ğŸ“Š Today's Training Stats:"

          if [ "$TABLE_EXISTS" -eq "0" ]; then
            echo "No stats available (table not created)"
          else
            Q2=$(cat <<'SQL'
            SELECT
              COUNT(DISTINCT athlete_id) AS athletes_trained,
              COUNT(*) AS total_models,
              ROUND(AVG(primary_rmse), 3) AS avg_rmse,
              FORMAT_TIMESTAMP('%Y-%m-%d %H:%M', MAX(trained_at)) AS last_run
            FROM `${GCP_PROJECT}.${BQ_DATASET}.model_training_summary`
            WHERE DATE(trained_at) = CURRENT_DATE()
            SQL
            )
            Q2="$(envsubst <<<"$Q2")"
            bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT" query --use_legacy_sql=false "$Q2" 2>&1 || echo "No stats available"
          fi

  summary:
    runs-on: ubuntu-latest
    needs: [ingest, check-readiness, train-models]
    if: always()
    steps:
      - name: Pipeline Summary
        env:
          HAS_MATCHES:  ${{ needs.check-readiness.outputs.has_matches }}
          MATCH_COUNT:  ${{ needs.check-readiness.outputs.match_count }}
          ATH_COUNT:    ${{ needs.check-readiness.outputs.athlete_count }}
        run: |
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "    WORKLOAD & MODEL PIPELINE SUMMARY"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo ""
          echo "ğŸ”¹ Workload Ingest:     ${{ needs.ingest.result }}"
          echo "ğŸ”¹ Readiness Check:     ${{ needs.check-readiness.result }}"
          echo "ğŸ”¹ Model Training:      ${{ needs.train-models.result }}"
          echo ""

          if [ "${{ needs.ingest.result }}" != "success" ]; then
            echo "âŒ INGEST FAILED - Check workload ingest logs"
            exit 1

          elif [ "${{ needs.check-readiness.result }}" == "skipped" ]; then
            echo "â­ï¸  PIPELINE SKIPPED - Ingest failed, no readiness check performed"
            exit 1

          elif [ "$HAS_MATCHES" != "true" ]; then
            echo "âœ… INGEST COMPLETE - Workload data saved"
            echo "â­ï¸  TRAINING SKIPPED - No matching readiness data found (OK)"
            exit 0

          elif [ "${{ needs.train-models.result }}" == "skipped" ]; then
            echo "âœ… INGEST COMPLETE - Workload data saved"
            echo "âœ… READINESS CHECK - Found $MATCH_COUNT matches across $ATH_COUNT athletes"
            echo "â­ï¸  TRAINING SKIPPED - Manual skip requested"
            exit 0

          elif [ "${{ needs.train-models.result }}" == "success" ]; then
            echo "âœ… PIPELINE COMPLETE - All jobs succeeded!"
            echo ""
            echo "ğŸ“Š Summary:"
            echo "   â€¢ Matches found: $MATCH_COUNT"
            echo "   â€¢ Athletes trained: $ATH_COUNT"
            exit 0

          elif [ "${{ needs.train-models.result }}" == "failure" ]; then
            echo "âœ… INGEST COMPLETE - Workload data saved"
            echo "âœ… READINESS CHECK - Found $MATCH_COUNT matches"
            echo "âŒ TRAINING FAILED - Check model training logs"
            exit 0

          else
            echo "âš ï¸  UNKNOWN STATUS - Check individual job logs"
            exit 1

          fi
