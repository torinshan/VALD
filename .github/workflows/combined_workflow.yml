name: Workload & Model Pipeline (Every 15 min)

on:
  schedule:
    - cron: "*/15 * * * *"
  workflow_dispatch:
    inputs:
      skip_training:
        description: "Skip model training (ingest only)"
        required: false
        type: boolean
        default: false
      start_date:
        description: "Model training start date (YYYY-MM-DD)"
        required: false
        default: "2025-06-01"
      end_date:
        description: "Model training end date (YYYY-MM-DD)"
        required: false
        default: "2025-12-31"

env:
  # Workload & Model Project (new - destination for writes)
  GCP_PROJECT_ML: sac-ml-models
  BQ_DATASET: analytics
  BQ_LOCATION: US

  # Readiness Data Project (original - source for reads)
  GCP_PROJECT_READINESS: sac-vald-hub

  WORKLOAD_TABLE: workload_daily
  READINESS_TABLE: vald_fd_jumps
  ROSTER_TABLE: roster_mapping
  PREDICTIONS_TABLE: readiness_predictions_byname

  TEAM_NAME: sacstate-football

permissions:
  contents: read

jobs:
  ingest:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          lfs: true
          fetch-depth: 1

      - name: Auth to Google Cloud (ML Project - for writes)
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY_ML }}
          create_credentials_file: true
          export_environment_variables: true

      - uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.GCP_PROJECT_ML }}

      - uses: r-lib/actions/setup-r@v2
        with:
          use-public-rspm: true

      - name: Install R dependencies
        uses: r-lib/actions/setup-r-dependencies@v2
        with:
          cache-version: 1
          extra-packages: |
            any::bigrquery
            any::DBI
            any::dplyr
            any::tidyr
            any::readr
            any::readxl
            any::stringr
            any::purrr
            any::tibble
            any::data.table
            any::hms
            any::lubridate
            any::httr
            any::jsonlite
            any::curl
            any::gargle
            any::glue
            any::slider
            any::janitor

      - name: System libs
        run: |
          sudo apt-get update
          sudo apt-get install -y libcurl4-openssl-dev libssl-dev libxml2-dev jq

      - name: Verify workload file exists
        run: |
          test -f ".github/Clean_Activities_GPS.xlsx" || (echo "File not found: .github/Clean_Activities_GPS.xlsx"; exit 1)
          ls -lh ".github/Clean_Activities_GPS.xlsx"

      - name: Ingest workload (local xlsx)
        env:
          GCP_PROJECT: ${{ env.GCP_PROJECT_ML }}
          BQ_DATASET: ${{ env.BQ_DATASET }}
          BQ_LOCATION: ${{ env.BQ_LOCATION }}
          BQ_TABLE: ${{ env.WORKLOAD_TABLE }}
          BQ_WRITE_MODE: MERGE
          LOCAL_FILE_PATH: ".github/Clean_Activities_GPS.xlsx"
          ONEDRIVE_PUBLIC_URL: ""
        run: Rscript ".github/scripts/workload_ingest.R"

      - name: Check ingest success
        id: ingest_check
        run: |
          echo "âœ… Workload ingest completed successfully"
          echo "status=success" >> "$GITHUB_OUTPUT"

    outputs:
      status: ${{ steps.ingest_check.outputs.status }}

  ingest-roster:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          lfs: true
          fetch-depth: 1

      - name: Auth to Google Cloud (ML Project - for writes)
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY_ML }}
          create_credentials_file: true
          export_environment_variables: true

      - uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.GCP_PROJECT_ML }}

      - uses: r-lib/actions/setup-r@v2
        with:
          use-public-rspm: true

      - name: Install R dependencies (roster)
        uses: r-lib/actions/setup-r-dependencies@v2
        with:
          cache-version: 1
          extra-packages: |
            any::bigrquery
            any::DBI
            any::dplyr
            any::readxl
            any::stringr
            any::tibble
            any::janitor

      - name: System libs
        run: |
          sudo apt-get update
          sudo apt-get install -y libcurl4-openssl-dev libssl-dev libxml2-dev jq

      - name: Verify roster file exists
        run: |
          test -f ".github/Sac State Roster - Summer 2025.xlsx" || (echo "File not found: .github/Sac State Roster - Summer 2025.xlsx"; exit 1)
          ls -lh ".github/Sac State Roster - Summer 2025.xlsx"

      - name: Ingest roster (local xlsx â†’ BigQuery)
        env:
          GCP_PROJECT:        ${{ env.GCP_PROJECT_ML }}
          BQ_DATASET:         ${{ env.BQ_DATASET }}
          BQ_LOCATION:        ${{ env.BQ_LOCATION }}
          ROSTER_TABLE:       ${{ env.ROSTER_TABLE }}
          ROSTER_LOCAL_FILE:  ".github/Sac State Roster - Summer 2025.xlsx"
          ROSTER_SHEET:       "Master"
        run: Rscript ".github/scripts/roster_ingest.R"

      - name: Check roster ingest success
        id: roster_check
        run: |
          echo "âœ… Roster ingest completed successfully"
          echo "status=success" >> "$GITHUB_OUTPUT"

    outputs:
      status: ${{ steps.roster_check.outputs.status }}

  check-readiness:
    runs-on: ubuntu-latest
    needs: [ingest, ingest-roster]
    if: needs.ingest.outputs.status == 'success' && needs['ingest-roster'].outputs.status == 'success'

    steps:
      - uses: actions/checkout@v4

      # FIX 1: First authenticate to READINESS project for reading VALD data
      - name: Auth to Readiness Project (for reading VALD data)
        uses: google-github-actions/auth@v2
        id: auth_readiness
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY_READINESS }}
          create_credentials_file: true
          cleanup_credentials: false

      - name: Setup gcloud for Readiness Project
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.GCP_PROJECT_READINESS }}

      # Save readiness credentials path for later use
      - name: Save readiness credentials path
        run: |
          echo "GOOGLE_APPLICATION_CREDENTIALS_READINESS=${{ steps.auth_readiness.outputs.credentials_file_path }}" >> $GITHUB_ENV

      # FIX 2: Then authenticate to ML project (will be used as default)
      - name: Auth to Google Cloud (ML Project - primary)
        uses: google-github-actions/auth@v2
        id: auth_ml
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY_ML }}
          create_credentials_file: true
          export_environment_variables: true

      - name: Setup gcloud for ML Project
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.GCP_PROJECT_ML }}

      - name: Ensure jq + envsubst present
        run: |
          if ! command -v jq >/dev/null 2>&1; then
            sudo apt-get update && sudo apt-get install -y jq
          fi
          if ! command -v envsubst >/dev/null 2>&1; then
            sudo apt-get update && sudo apt-get install -y gettext-base
          fi

      - name: Sync readiness table from sac-vald-hub to sac-ml-models
        id: sync_readiness
        shell: bash
        env:
          GCP_PROJECT_ML: ${{ env.GCP_PROJECT_ML }}
          GCP_PROJECT_READINESS: ${{ env.GCP_PROJECT_READINESS }}
          BQ_DATASET: ${{ env.BQ_DATASET }}
          BQ_LOCATION: ${{ env.BQ_LOCATION }}
          READINESS_TABLE: ${{ env.READINESS_TABLE }}
        run: |
          set -euo pipefail

          echo "ğŸ”„ Syncing readiness table from source to ML project..."
          echo "  Source:      ${GCP_PROJECT_READINESS}.${BQ_DATASET}.${READINESS_TABLE}"
          echo "  Destination: ${GCP_PROJECT_ML}.${BQ_DATASET}.${READINESS_TABLE}"
          echo ""

          # FIX 3: Use the readiness credentials explicitly for source queries
          echo "ğŸ“‹ Checking source table in ${GCP_PROJECT_READINESS}..."

          # Activate readiness service account
          gcloud auth activate-service-account --key-file="${GOOGLE_APPLICATION_CREDENTIALS_READINESS}"

          # Try to show the table and capture the output
          set +e  # Temporarily disable exit on error
          BQ_SHOW_OUTPUT=$(bq --location="$BQ_LOCATION" \
               --project_id="$GCP_PROJECT_READINESS" \
               --credential_file="${GOOGLE_APPLICATION_CREDENTIALS_READINESS}" \
               show "${GCP_PROJECT_READINESS}:${BQ_DATASET}.${READINESS_TABLE}" 2>&1)
          BQ_SHOW_EXIT_CODE=$?
          set -e  # Re-enable exit on error

          if [ $BQ_SHOW_EXIT_CODE -ne 0 ]; then
            # Check if it's a "Not found" error, permission error, or other errors
            if echo "$BQ_SHOW_OUTPUT" | grep -qi "not found\|does not exist"; then
              echo "âš ï¸  Source table ${GCP_PROJECT_READINESS}.${BQ_DATASET}.${READINESS_TABLE} not found"
              echo "   This table is populated by the saturday_makeup workflow."
              echo "   Sync will be skipped - using existing local copy if available."
              echo ""
              echo "sync_action=skipped_no_source" >> "$GITHUB_OUTPUT"
              exit 0
            elif echo "$BQ_SHOW_OUTPUT" | grep -qi "permission.*denied\|access denied"; then
              # Permission error - skip sync gracefully
              echo "âš ï¸  Permission denied accessing ${GCP_PROJECT_READINESS}.${BQ_DATASET}.${READINESS_TABLE}"
              echo "   Service account does not have read access to the source table."
              echo "   Sync will be skipped - training will use local data if available."
              echo ""
              echo "Error details:"
              echo "$BQ_SHOW_OUTPUT"
              echo ""
              echo "To fix this issue, grant read permissions to the service account:"
              echo "gcloud projects add-iam-policy-binding ${GCP_PROJECT_READINESS} \\"
              echo "  --member='serviceAccount:gha-bq@sac-vald-hub.iam.gserviceaccount.com' \\"
              echo "  --role='roles/bigquery.dataViewer'"
              echo ""
              echo "sync_action=skipped_no_permission" >> "$GITHUB_OUTPUT"
              exit 0
            else
              # Other error (location, authentication, etc.)
              echo "âŒ Failed to check source table ${GCP_PROJECT_READINESS}.${BQ_DATASET}.${READINESS_TABLE}"
              echo ""
              echo "Error output:"
              echo "$BQ_SHOW_OUTPUT"
              echo ""
              echo "Possible issues:"
              echo "  1. Location mismatch (currently using: $BQ_LOCATION)"
              echo "  2. Service account authentication failed"
              echo "  3. Dataset or table name is incorrect"
              echo ""
              echo "sync_action=skipped_error" >> "$GITHUB_OUTPUT"
              exit 0
            fi
          fi

          # Get source table metadata using readiness credentials
          SOURCE_INFO=$(bq --location="$BQ_LOCATION" \
                          --project_id="$GCP_PROJECT_READINESS" \
                          --credential_file="${GOOGLE_APPLICATION_CREDENTIALS_READINESS}" \
                          show --format=json "${GCP_PROJECT_READINESS}:${BQ_DATASET}.${READINESS_TABLE}")
          SOURCE_NUM_ROWS=$(echo "$SOURCE_INFO" | jq -r '.numRows // "0"')
          SOURCE_SCHEMA_FIELDS=$(echo "$SOURCE_INFO" | jq -r '.schema.fields | length')

          # Get most recent date from source table
          SOURCE_MAX_DATE=$(bq --location="$BQ_LOCATION" \
                              --project_id="$GCP_PROJECT_READINESS" \
                              --credential_file="${GOOGLE_APPLICATION_CREDENTIALS_READINESS}" \
                              query --use_legacy_sql=false --format=csv \
                              "SELECT MAX(DATE(date)) as max_date FROM \`${GCP_PROJECT_READINESS}.${BQ_DATASET}.${READINESS_TABLE}\`" \
                              2>/dev/null | tail -n1)

          echo "  Source stats:"
          echo "    Rows: $SOURCE_NUM_ROWS"
          echo "    Columns: $SOURCE_SCHEMA_FIELDS"
          echo "    Latest date: ${SOURCE_MAX_DATE:-none}"
          echo ""

          # FIX 4: Switch back to ML project credentials for destination operations
          gcloud auth activate-service-account --key-file="${GOOGLE_APPLICATION_CREDENTIALS}"
          gcloud config set project "$GCP_PROJECT_ML"

          # Check if destination table exists in sac-ml-models
          echo "ğŸ“‹ Checking destination table in ${GCP_PROJECT_ML}..."

          if bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" \
               show "${GCP_PROJECT_ML}:${BQ_DATASET}.${READINESS_TABLE}" >/dev/null 2>&1; then

            # Destination table exists - check if sync is needed
            DEST_INFO=$(bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" \
                          show --format=json "${GCP_PROJECT_ML}:${BQ_DATASET}.${READINESS_TABLE}")
            DEST_NUM_ROWS=$(echo "$DEST_INFO" | jq -r '.numRows // "0"')
            DEST_SCHEMA_FIELDS=$(echo "$DEST_INFO" | jq -r '.schema.fields | length')

            # Get most recent date from destination table
            DEST_MAX_DATE=$(bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" \
                              query --use_legacy_sql=false --format=csv \
                              "SELECT MAX(DATE(date)) as max_date FROM \`${GCP_PROJECT_ML}.${BQ_DATASET}.${READINESS_TABLE}\`" \
                              2>/dev/null | tail -n1)

            echo "  Destination stats:"
            echo "    Rows: $DEST_NUM_ROWS"
            echo "    Columns: $DEST_SCHEMA_FIELDS"
            echo "    Latest date: ${DEST_MAX_DATE:-none}"
            echo ""

            # Compare metadata
            if [ "$SOURCE_MAX_DATE" = "$DEST_MAX_DATE" ] && [ "$SOURCE_SCHEMA_FIELDS" = "$DEST_SCHEMA_FIELDS" ]; then
              echo "âœ… Tables are in sync (same latest date and column count)"
              echo "   Keeping existing copy in ${GCP_PROJECT_ML}"
              echo "sync_action=skipped" >> "$GITHUB_OUTPUT"
              exit 0
            else
              echo "âš ï¸  Tables are out of sync:"
              if [ "$SOURCE_MAX_DATE" != "$DEST_MAX_DATE" ]; then
                echo "   Latest dates differ: source=$SOURCE_MAX_DATE vs dest=$DEST_MAX_DATE"
              fi
              if [ "$SOURCE_SCHEMA_FIELDS" != "$DEST_SCHEMA_FIELDS" ]; then
                echo "   Column counts differ: source=$SOURCE_SCHEMA_FIELDS vs dest=$DEST_SCHEMA_FIELDS"
              fi
              echo "   â†’ Will update destination table"
              echo ""

              # Delete old table
              echo "ğŸ—‘ï¸  Deleting outdated table in ${GCP_PROJECT_ML}..."
              bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" \
                rm -f -t "${GCP_PROJECT_ML}:${BQ_DATASET}.${READINESS_TABLE}"
            fi
          else
            echo "â„¹ï¸  Destination table does not exist"
            echo "   â†’ Will create new copy"
            echo ""
          fi

          # Copy table from source to destination using bq cp
          echo "ğŸ“¦ Copying table from ${GCP_PROJECT_READINESS} to ${GCP_PROJECT_ML}..."

          # FIX 5: Use gcloud to copy with explicit project specifications
          # This requires the service account to have permissions on BOTH projects
          gcloud auth activate-service-account --key-file="${GOOGLE_APPLICATION_CREDENTIALS}"

          bq --location="$BQ_LOCATION" \
            --project_id="$GCP_PROJECT_ML" \
            cp -f \
            "${GCP_PROJECT_READINESS}:${BQ_DATASET}.${READINESS_TABLE}" \
            "${GCP_PROJECT_ML}:${BQ_DATASET}.${READINESS_TABLE}"

          # Verify copy success
          FINAL_INFO=$(bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" \
                        show --format=json "${GCP_PROJECT_ML}:${BQ_DATASET}.${READINESS_TABLE}")
          FINAL_ROWS=$(echo "$FINAL_INFO" | jq -r '.numRows // "0"')

          echo ""
          echo "âœ… Copy complete!"
          echo "   Final row count in ${GCP_PROJECT_ML}: $FINAL_ROWS"
          echo "sync_action=synced" >> "$GITHUB_OUTPUT"

      - name: Check for matching workload & readiness data
        id: check_matches
        shell: bash
        env:
          GCP_PROJECT_ML: ${{ env.GCP_PROJECT_ML }}
          BQ_DATASET: ${{ env.BQ_DATASET }}
          BQ_LOCATION: ${{ env.BQ_LOCATION }}
          WORKLOAD_TABLE: ${{ env.WORKLOAD_TABLE }}
          READINESS_TABLE: ${{ env.READINESS_TABLE }}
          ROSTER_TABLE: ${{ env.ROSTER_TABLE }}
        run: |
          set -euo pipefail

          echo "ğŸ” Checking for matching workload & readiness data..."
          echo ""

          # Check if readiness table exists (might be skipped if sync failed)
          if ! bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" \
               show "${GCP_PROJECT_ML}:${BQ_DATASET}.${READINESS_TABLE}" >/dev/null 2>&1; then
            echo "âš ï¸  No readiness table in ${GCP_PROJECT_ML}"
            echo "   â†’ Skipping training (no readiness data available)"
            echo "has_matches=false" >> "$GITHUB_OUTPUT"
            echo "match_count=0" >> "$GITHUB_OUTPUT"
            echo "athlete_count=0" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          # Query to find matches
          MATCH_SQL='
          WITH workload AS (
            SELECT DISTINCT
              w.roster_name as official_id,
              DATE(w.date) as date
            FROM `${GCP_PROJECT_ML}.${BQ_DATASET}.${WORKLOAD_TABLE}` w
            WHERE w.date >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY)
          ),
          readiness AS (
            SELECT DISTINCT
              r.full_name as vald_name,
              DATE(r.date) as date
            FROM `${GCP_PROJECT_ML}.${BQ_DATASET}.${READINESS_TABLE}` r
            WHERE r.date >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY)
          ),
          roster AS (
            SELECT official_id, vald_name
            FROM `${GCP_PROJECT_ML}.${BQ_DATASET}.${ROSTER_TABLE}`
          )
          SELECT
            COUNT(*) as match_count,
            COUNT(DISTINCT w.official_id) as athlete_count
          FROM workload w
          JOIN roster ros ON w.official_id = ros.official_id
          JOIN readiness r ON ros.vald_name = r.vald_name AND w.date = r.date
          '

          # Substitute environment variables
          MATCH_SQL="$(envsubst <<<"$MATCH_SQL")"

          # Execute query
          MATCH_RESULT=$(bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" \
                          query --use_legacy_sql=false --format=csv "$MATCH_SQL" | tail -n1)

          MATCH_COUNT=$(echo "$MATCH_RESULT" | cut -d',' -f1)
          ATHLETE_COUNT=$(echo "$MATCH_RESULT" | cut -d',' -f2)

          echo "ğŸ“Š Match Results:"
          echo "   Matching records: $MATCH_COUNT"
          echo "   Athletes with data: $ATHLETE_COUNT"
          echo ""

          if [ "$MATCH_COUNT" -gt 0 ]; then
            echo "âœ… Found matching data â†’ Training will proceed"
            echo "has_matches=true" >> "$GITHUB_OUTPUT"
          else
            echo "âš ï¸  No matching data â†’ Training will be skipped"
            echo "has_matches=false" >> "$GITHUB_OUTPUT"
          fi

          echo "match_count=$MATCH_COUNT" >> "$GITHUB_OUTPUT"
          echo "athlete_count=$ATHLETE_COUNT" >> "$GITHUB_OUTPUT"

    outputs:
      has_matches: ${{ steps.check_matches.outputs.has_matches }}
      match_count: ${{ steps.check_matches.outputs.match_count }}
      athlete_count: ${{ steps.check_matches.outputs.athlete_count }}
      sync_action: ${{ steps.sync_readiness.outputs.sync_action }}

  train-models:
    runs-on: ubuntu-latest
    needs: [check-readiness]
    if: |
      needs.check-readiness.outputs.has_matches == 'true' &&
      github.event.inputs.skip_training != 'true'

    steps:
      - uses: actions/checkout@v4

      # FIX 6: Authenticate to BOTH projects for R script
      - name: Auth to Readiness Project (for reading VALD data)
        uses: google-github-actions/auth@v2
        id: auth_readiness
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY_READINESS }}
          create_credentials_file: true
          cleanup_credentials: false

      - name: Auth to ML Project (for writes)
        uses: google-github-actions/auth@v2
        id: auth_ml
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY_ML }}
          create_credentials_file: true
          export_environment_variables: true

      - uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.GCP_PROJECT_ML }}

      # FIX 7: Export both credential paths to R script
      - name: Setup dual-project credentials for R
        run: |
          echo "GOOGLE_APPLICATION_CREDENTIALS_READINESS=${{ steps.auth_readiness.outputs.credentials_file_path }}" >> $GITHUB_ENV
          echo "GOOGLE_APPLICATION_CREDENTIALS=${{ steps.auth_ml.outputs.credentials_file_path }}" >> $GITHUB_ENV

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install scikit-learn==1.5.1 numpy==1.26.4

      - uses: r-lib/actions/setup-r@v2
        with:
          use-public-rspm: true

      - uses: r-lib/actions/setup-r-dependencies@v2
        with:
          cache-version: 1
          extra-packages: |
            any::bigrquery
            any::DBI
            any::dplyr
            any::tidyr
            any::data.table
            any::lubridate
            any::slider
            any::stringr
            any::gargle
            any::glue
            any::digest
            any::glmnet
            any::bnlearn
            any::reticulate

      - name: System libs
        run: |
          sudo apt-get update
          sudo apt-get install -y libcurl4-openssl-dev libssl-dev libxml2-dev jq

      - name: Verify BigQuery write access
        env:
          GCP_PROJECT_ML: ${{ env.GCP_PROJECT_ML }}
          BQ_DATASET:  ${{ env.BQ_DATASET }}
          BQ_LOCATION: ${{ env.BQ_LOCATION }}
        run: |
          set -euo pipefail
          echo "=== BigQuery Access Validation ==="
          echo "Checking access to dataset ${BQ_DATASET} in ML project ${GCP_PROJECT_ML}..."

          if ! bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" ls "${BQ_DATASET}" >/dev/null 2>&1; then
            echo ""
            echo "âŒ Cannot list dataset ${BQ_DATASET}"
            echo ""
            echo "Troubleshooting:"
            echo "  1. Service account needs BigQuery Data Editor permission"
            echo "  2. Or roles/bigquery.dataEditor on the dataset"
            echo "  3. Verify GCP_SA_KEY_ML has BigQuery write access"
            echo ""
            echo "Attempting to show more details:"
            bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" ls 2>&1 || true
            exit 1
          fi
          echo "âœ… BigQuery dataset accessible"

          # Verify we can query the dataset
          if ! bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" query --use_legacy_sql=false "SELECT 1" >/dev/null 2>&1; then
            echo "âŒ Cannot execute queries in BigQuery"
            exit 1
          fi
          echo "âœ… BigQuery write access verified"

      - name: Run model training
        env:
          GCP_PROJECT: ${{ env.GCP_PROJECT_ML }}
          GCP_PROJECT_READINESS: ${{ env.GCP_PROJECT_READINESS }}
          BQ_DATASET:  ${{ env.BQ_DATASET }}
          BQ_LOCATION: ${{ env.BQ_LOCATION }}
          TEAM_NAME:   ${{ env.TEAM_NAME }}

          WORKLOAD_TABLE:    ${{ env.WORKLOAD_TABLE }}
          READINESS_TABLE:   ${{ env.READINESS_TABLE }}
          ROSTER_TABLE:      ${{ env.ROSTER_TABLE }}
          PREDICTIONS_TABLE: ${{ env.PREDICTIONS_TABLE }}

          START_DATE:        ${{ github.event.inputs.start_date || '2025-06-01' }}
          END_DATE:          ${{ github.event.inputs.end_date   || '2025-12-31' }}
          TRAIN_FRACTION:    '0.80'
          MIN_TRAIN_OBS:     '3'

          HAS_MATCHES:       ${{ needs.check-readiness.outputs.has_matches }}
          MATCH_LOOKBACK_DAYS: '7'
          SKIP_TRAINING:     ${{ github.event.inputs.skip_training || 'false' }}
          SKIP_REGISTRY_STAGES: 'true'  # BigQuery free tier doesn't support DML (INSERT) queries
        run: Rscript ".github/scripts/readiness_models_cloud.R"

      - name: Show today's training summary (best-effort)
        if: success()
        env:
          GCP_PROJECT_ML: ${{ env.GCP_PROJECT_ML }}
          BQ_DATASET:  ${{ env.BQ_DATASET }}
          BQ_LOCATION: ${{ env.BQ_LOCATION }}
        run: |
          set -eo pipefail
          if ! command -v envsubst >/dev/null 2>&1; then
            sudo apt-get update && sudo apt-get install -y gettext-base
          fi

          # Check if model_training_summary table exists before querying
          TABLE_EXISTS=$(bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" \
            ls -n 1000 "${BQ_DATASET}" 2>/dev/null | grep -c "model_training_summary" || echo "0")

          echo ""
          echo "ğŸ“ˆ Latest Models:"

          if [ "$TABLE_EXISTS" -eq "0" ]; then
            echo "No training results yet (table not created)"
          else
            # Use multi-line string instead of heredoc for proper YAML syntax
            Q1='
            SELECT
              athlete_id,
              roster_name,
              model_type,
              ROUND(primary_rmse, 3) AS rmse,
              n_train,
              FORMAT_TIMESTAMP('%H:%M', trained_at) AS time
            FROM `${GCP_PROJECT_ML}.${BQ_DATASET}.model_training_summary`
            WHERE DATE(trained_at) = CURRENT_DATE()
            ORDER BY trained_at DESC
            LIMIT 10
            '
            Q1="$(envsubst <<<"$Q1")"
            bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" query --use_legacy_sql=false --max_rows=10 "$Q1" 2>&1 || echo "No training results yet"
          fi

          echo ""
          echo "ğŸ“Š Today's Training Stats:"

          if [ "$TABLE_EXISTS" -eq "0" ]; then
            echo "No stats available (table not created)"
          else
            Q2='
            SELECT
              COUNT(DISTINCT athlete_id) AS athletes_trained,
              COUNT(*) AS total_models,
              ROUND(AVG(primary_rmse), 3) AS avg_rmse,
              FORMAT_TIMESTAMP('%H:%M', MAX(trained_at)) AS last_run
            FROM `${GCP_PROJECT_ML}.${BQ_DATASET}.model_training_summary`
            WHERE DATE(trained_at) = CURRENT_DATE()
            '
            Q2="$(envsubst <<<"$Q2")"
            bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" query --use_legacy_sql=false "$Q2" 2>&1 || echo "No stats available"
          fi

      - name: Cleanup credentials
        if: always()
        run: |
          if [ -n "${GOOGLE_APPLICATION_CREDENTIALS_READINESS:-}" ] && [ -f "${GOOGLE_APPLICATION_CREDENTIALS_READINESS}" ]; then
            rm -f "${GOOGLE_APPLICATION_CREDENTIALS_READINESS}"
          fi

  summary:
    runs-on: ubuntu-latest
    needs: [ingest, check-readiness, train-models]
    if: always()
    steps:
      - name: Pipeline Summary
        env:
          HAS_MATCHES:  ${{ needs.check-readiness.outputs.has_matches }}
          MATCH_COUNT:  ${{ needs.check-readiness.outputs.match_count }}
          ATH_COUNT:    ${{ needs.check-readiness.outputs.athlete_count }}
        run: |
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "    WORKLOAD & MODEL PIPELINE SUMMARY"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo ""
          echo "ğŸ”¹ Workload Ingest:     ${{ needs.ingest.result }}"
          echo "ğŸ”¹ Readiness Check:     ${{ needs.check-readiness.result }}"
          echo "ğŸ”¹ Model Training:      ${{ needs.train-models.result }}"
          echo ""

          if [ "${{ needs.ingest.result }}" != "success" ]; then
            echo "âŒ INGEST FAILED - Check workload ingest logs"
            exit 1

          elif [ "${{ needs.check-readiness.result }}" == "skipped" ]; then
            echo "â­ï¸  PIPELINE SKIPPED - Ingest failed, no readiness check performed"
            exit 1

          elif [ "$HAS_MATCHES" != "true" ]; then
            echo "âœ… INGEST COMPLETE - Workload data saved"
            echo "â­ï¸  TRAINING SKIPPED - No matching readiness data found (OK)"
            exit 0

          elif [ "${{ needs.train-models.result }}" == "skipped" ]; then
            echo "âœ… INGEST COMPLETE - Workload data saved"
            echo "âœ… READINESS CHECK - Found $MATCH_COUNT matches across $ATH_COUNT athletes"
            echo "â­ï¸  TRAINING SKIPPED - Manual skip requested"
            exit 0

          elif [ "${{ needs.train-models.result }}" == "success" ]; then
            echo "âœ… PIPELINE COMPLETE - All jobs succeeded!"
            echo ""
            echo "ğŸ“Š Summary:"
            echo "   â€¢ Matches found: $MATCH_COUNT"
            echo "   â€¢ Athletes trained: $ATH_COUNT"
            exit 0

          elif [ "${{ needs.train-models.result }}" == "failure" ]; then
            echo "âœ… INGEST COMPLETE - Workload data saved"
            echo "âœ… READINESS CHECK - Found $MATCH_COUNT matches"
            echo "âŒ TRAINING FAILED - Check model training logs"
            exit 0

          else
            echo "âš ï¸  UNKNOWN STATUS - Check individual job logs"
            exit 1

          fi
