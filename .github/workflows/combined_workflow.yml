name: Workload & Model Pipeline (Every 15 min)

on:
  schedule:
    - cron: "*/15 * * * *"
  workflow_dispatch:
    inputs:
      skip_training:
        description: "Skip model training (ingest only)"
        required: false
        type: boolean
        default: false
      start_date:
        description: "Model training start date (YYYY-MM-DD)"
        required: false
        default: "2025-03-01"
      end_date:
        description: "Model training end date (YYYY-MM-DD)"
        required: false
        default: "2026-12-31"

env:
  # Workload & Model Project (new - destination for writes)
  GCP_PROJECT_ML: sac-ml-models
  BQ_DATASET: analytics
  BQ_LOCATION: US

  # Readiness Data Project (original - source for reads)
  GCP_PROJECT_READINESS: sac-vald-hub

  WORKLOAD_TABLE: workload_daily
  READINESS_TABLE: vald_fd_jumps_test_copy
  ROSTER_TABLE: roster_mapping
  PREDICTIONS_TABLE: readiness_predictions_byname

  TEAM_NAME: sacstate-football

permissions:
  contents: read

jobs:
  ingest:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          lfs: true
          fetch-depth: 1

      - name: Auth to Google Cloud (ML Project - for writes)
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY_ML }}
          create_credentials_file: true
          export_environment_variables: true

      - uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.GCP_PROJECT_ML }}

      - uses: r-lib/actions/setup-r@v2
        with:
          use-public-rspm: true

      - name: Install R dependencies
        uses: r-lib/actions/setup-r-dependencies@v2
        with:
          cache-version: 1
          extra-packages: |
            any::bigrquery
            any::DBI
            any::dplyr
            any::tidyr
            any::readr
            any::readxl
            any::stringr
            any::purrr
            any::tibble
            any::data.table
            any::hms
            any::lubridate
            any::httr
            any::jsonlite
            any::curl
            any::gargle
            any::glue
            any::slider
            any::janitor

      - name: System libs
        run: |
          sudo apt-get update
          sudo apt-get install -y libcurl4-openssl-dev libssl-dev libxml2-dev jq

      - name: Verify workload file exists
        run: |
          test -f ".github/Clean_Activities_GPS.xlsx" || (echo "File not found: .github/Clean_Activities_GPS.xlsx"; exit 1)
          ls -lh ".github/Clean_Activities_GPS.xlsx"

      - name: Ingest workload (local xlsx)
        env:
          GCP_PROJECT: ${{ env.GCP_PROJECT_ML }}
          BQ_DATASET: ${{ env.BQ_DATASET }}
          BQ_LOCATION: ${{ env.BQ_LOCATION }}
          BQ_TABLE: ${{ env.WORKLOAD_TABLE }}
          BQ_WRITE_MODE: MERGE
          LOCAL_FILE_PATH: ".github/Clean_Activities_GPS.xlsx"
          ONEDRIVE_PUBLIC_URL: ""
        run: Rscript ".github/scripts/workload_ingest.R"

      - name: Check ingest success
        id: ingest_check
        run: |
          echo "âœ… Workload ingest completed successfully"
          echo "status=success" >> "$GITHUB_OUTPUT"

    outputs:
      status: ${{ steps.ingest_check.outputs.status }}

  ingest-roster:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          lfs: true
          fetch-depth: 1

      - name: Auth to Google Cloud (ML Project - for writes)
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY_ML }}
          create_credentials_file: true
          export_environment_variables: true

      - uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.GCP_PROJECT_ML }}

      - uses: r-lib/actions/setup-r@v2
        with:
          use-public-rspm: true

      - name: Install R dependencies (roster)
        uses: r-lib/actions/setup-r-dependencies@v2
        with:
          cache-version: 1
          extra-packages: |
            any::bigrquery
            any::DBI
            any::dplyr
            any::readxl
            any::stringr
            any::tibble
            any::janitor

      - name: System libs
        run: |
          sudo apt-get update
          sudo apt-get install -y libcurl4-openssl-dev libssl-dev libxml2-dev jq

      - name: Verify roster file exists
        run: |
          test -f ".github/Sac State Roster - Summer 2025.xlsx" || (echo "File not found: .github/Sac State Roster - Summer 2025.xlsx"; exit 1)
          ls -lh ".github/Sac State Roster - Summer 2025.xlsx"

      - name: Ingest roster (local xlsx â†’ BigQuery)
        env:
          GCP_PROJECT: ${{ env.GCP_PROJECT_ML }}
          BQ_DATASET: ${{ env.BQ_DATASET }}
          BQ_LOCATION: ${{ env.BQ_LOCATION }}
          ROSTER_TABLE: ${{ env.ROSTER_TABLE }}
          ROSTER_LOCAL_FILE: ".github/Sac State Roster - Summer 2025.xlsx"
          ROSTER_SHEET: "Master"
        run: Rscript ".github/scripts/roster_ingest.R"

      - name: Check roster ingest success
        id: roster_check
        run: |
          echo "âœ… Roster ingest completed successfully"
          echo "status=success" >> "$GITHUB_OUTPUT"

    outputs:
      status: ${{ steps.roster_check.outputs.status }}

  check-readiness:
    runs-on: ubuntu-latest
    needs: [ingest, ingest-roster]
    if: needs.ingest.outputs.status == 'success' && needs['ingest-roster'].outputs.status == 'success'

    steps:
      - uses: actions/checkout@v4

      # FIX 1: First authenticate to READINESS project for reading VALD data
      - name: Auth to Readiness Project (for reading VALD data)
        uses: google-github-actions/auth@v2
        id: auth_readiness
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY_READINESS }}
          create_credentials_file: true
          cleanup_credentials: false

      - name: Setup gcloud for Readiness Project
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.GCP_PROJECT_READINESS }}

      # Save readiness credentials path for later use
      - name: Save readiness credentials path
        run: |
          echo "GOOGLE_APPLICATION_CREDENTIALS_READINESS=${{ steps.auth_readiness.outputs.credentials_file_path }}" >> $GITHUB_ENV

      # FIX 2: Then authenticate to ML project (will be used as default)
      - name: Auth to Google Cloud (ML Project - primary)
        uses: google-github-actions/auth@v2
        id: auth_ml
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY_ML }}
          create_credentials_file: true
          export_environment_variables: true

      - name: Setup gcloud for ML Project
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.GCP_PROJECT_ML }}

      - name: Ensure jq + envsubst present
        run: |
          if ! command -v jq >/dev/null 2>&1; then
            sudo apt-get update && sudo apt-get install -y jq
          fi
          if ! command -v envsubst >/dev/null 2>&1; then
            sudo apt-get update && sudo apt-get install -y gettext-base
          fi

      - name: Check and sync readiness table if needed
        id: sync_readiness
        shell: bash
        env:
          GCP_PROJECT_ML: ${{ env.GCP_PROJECT_ML }}
          GCP_PROJECT_READINESS: ${{ env.GCP_PROJECT_READINESS }}
          BQ_DATASET: ${{ env.BQ_DATASET }}
          BQ_LOCATION: ${{ env.BQ_LOCATION }}
          READINESS_TABLE: ${{ env.READINESS_TABLE }}
        run: |
          set -euo pipefail

          echo "ğŸ” Checking if readiness data needs sync..."
          echo "  Source:      ${GCP_PROJECT_READINESS}.${BQ_DATASET}.${READINESS_TABLE}"
          echo "  Destination: ${GCP_PROJECT_ML}.${BQ_DATASET}.${READINESS_TABLE}"
          echo ""

          # Check source table in sac-vald-hub using readiness credentials
          echo "ğŸ“‹ Checking source table in ${GCP_PROJECT_READINESS}..."
          gcloud auth activate-service-account --key-file="${GOOGLE_APPLICATION_CREDENTIALS_READINESS}"
          
          set +e
          SOURCE_INFO=$(bq --location="$BQ_LOCATION" \
                          --project_id="$GCP_PROJECT_READINESS" \
                          --credential_file="${GOOGLE_APPLICATION_CREDENTIALS_READINESS}" \
                          show --format=json "${GCP_PROJECT_READINESS}:${BQ_DATASET}.${READINESS_TABLE}" 2>&1)
          SOURCE_EXIT_CODE=$?
          set -e

          if [ $SOURCE_EXIT_CODE -ne 0 ]; then
            echo "âš ï¸  Source table not accessible or doesn't exist"
            echo "   Will use existing data in ${GCP_PROJECT_ML} if available"
            echo "sync_action=skipped_no_source" >> "$GITHUB_OUTPUT"
          else
            SOURCE_NUM_ROWS=$(echo "$SOURCE_INFO" | jq -r '.numRows // "0"')
            SOURCE_MAX_DATE=$(bq --location="$BQ_LOCATION" \
                                --project_id="$GCP_PROJECT_READINESS" \
                                --credential_file="${GOOGLE_APPLICATION_CREDENTIALS_READINESS}" \
                                query --use_legacy_sql=false --format=csv \
                                "SELECT MAX(DATE(date)) as max_date FROM \`${GCP_PROJECT_READINESS}.${BQ_DATASET}.${READINESS_TABLE}\`" \
                                2>/dev/null | tail -n1)
            
            echo "  Source: ${SOURCE_NUM_ROWS} rows, latest date: ${SOURCE_MAX_DATE:-none}"

            # Check destination table in sac-ml-models using ML credentials
            echo "ğŸ“‹ Checking destination table in ${GCP_PROJECT_ML}..."
            gcloud auth activate-service-account --key-file="${GOOGLE_APPLICATION_CREDENTIALS}"
            
            if bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" \
                 show "${GCP_PROJECT_ML}:${BQ_DATASET}.${READINESS_TABLE}" >/dev/null 2>&1; then
              
              DEST_INFO=$(bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" \
                            show --format=json "${GCP_PROJECT_ML}:${BQ_DATASET}.${READINESS_TABLE}")
              DEST_NUM_ROWS=$(echo "$DEST_INFO" | jq -r '.numRows // "0"')
              DEST_MAX_DATE=$(bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" \
                                query --use_legacy_sql=false --format=csv \
                                "SELECT MAX(DATE(date)) as max_date FROM \`${GCP_PROJECT_ML}.${BQ_DATASET}.${READINESS_TABLE}\`" \
                                2>/dev/null | tail -n1)
              
              echo "  Destination: ${DEST_NUM_ROWS} rows, latest date: ${DEST_MAX_DATE:-none}"

              # Compare dates to determine if sync is needed
              if [ "$SOURCE_MAX_DATE" = "$DEST_MAX_DATE" ]; then
                echo "âœ… Data is up-to-date (latest date matches)"
                echo "   Proceeding with existing data in ${GCP_PROJECT_ML}"
                echo "sync_action=current" >> "$GITHUB_OUTPUT"
              else
                echo "âš ï¸  Data is outdated (source: $SOURCE_MAX_DATE vs dest: $DEST_MAX_DATE)"
                echo "ğŸ“¦ Syncing updated data..."
                
                bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" \
                  cp -f \
                  "${GCP_PROJECT_READINESS}:${BQ_DATASET}.${READINESS_TABLE}" \
                  "${GCP_PROJECT_ML}:${BQ_DATASET}.${READINESS_TABLE}"
                
                echo "âœ… Sync complete!"
                echo "sync_action=synced" >> "$GITHUB_OUTPUT"
              fi
            else
              echo "â„¹ï¸  Destination table doesn't exist - creating initial copy..."
              
              bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" \
                cp -f \
                "${GCP_PROJECT_READINESS}:${BQ_DATASET}.${READINESS_TABLE}" \
                "${GCP_PROJECT_ML}:${BQ_DATASET}.${READINESS_TABLE}"
              
              echo "âœ… Initial copy created!"
              echo "sync_action=created" >> "$GITHUB_OUTPUT"
            fi
          fi

      - name: Check for matching workload & readiness data
        id: check_matches
        shell: bash
        env:
          GCP_PROJECT_ML: ${{ env.GCP_PROJECT_ML }}
          BQ_DATASET: ${{ env.BQ_DATASET }}
          BQ_LOCATION: ${{ env.BQ_LOCATION }}
          WORKLOAD_TABLE: ${{ env.WORKLOAD_TABLE }}
          READINESS_TABLE: ${{ env.READINESS_TABLE }}
          ROSTER_TABLE: ${{ env.ROSTER_TABLE }}
        run: |
          set -euo pipefail
          
          echo "ğŸ” Checking for matching workload & readiness data..."
          echo "  Note: All data is in ${GCP_PROJECT_ML}"
          echo ""

          # Check if readiness table exists in ML project
          if ! bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" \
               show "${GCP_PROJECT_ML}:${BQ_DATASET}.${READINESS_TABLE}" >/dev/null 2>&1; then
            echo "âš ï¸  No readiness table in ${GCP_PROJECT_ML}"
            echo "   Skipping training (no readiness data available)"
            echo "has_matches=false" >> "$GITHUB_OUTPUT"
            echo "match_count=0" >> "$GITHUB_OUTPUT"
            echo "athlete_count=0" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          # Query to find matches - all data in ML project
          MATCH_SQL='
          WITH workload AS (
            SELECT DISTINCT 
              w.roster_name as official_id,
              DATE(w.date) as date
            FROM `${GCP_PROJECT_ML}.${BQ_DATASET}.${WORKLOAD_TABLE}` w
            WHERE w.date >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY)
          ),
          readiness AS (
            SELECT DISTINCT
              r.full_name as vald_name,
              DATE(r.date) as date
            FROM `${GCP_PROJECT_ML}.${BQ_DATASET}.${READINESS_TABLE}` r
            WHERE r.date >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY)
          ),
          roster AS (
            SELECT official_id, vald_name
            FROM `${GCP_PROJECT_ML}.${BQ_DATASET}.${ROSTER_TABLE}`
          )
          SELECT 
            COUNT(*) as match_count,
            COUNT(DISTINCT w.official_id) as athlete_count
          FROM workload w
          JOIN roster ros ON w.official_id = ros.official_id
          JOIN readiness r ON ros.vald_name = r.vald_name AND w.date = r.date
          '

          # Substitute environment variables
          MATCH_SQL="$(envsubst '${GCP_PROJECT_ML} ${BQ_DATASET} ${WORKLOAD_TABLE} ${READINESS_TABLE} ${ROSTER_TABLE}' <<<"$MATCH_SQL")"

          # Execute query
          set +e
          MATCH_RESULT=$(bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" \
                          query --use_legacy_sql=false --format=csv "$MATCH_SQL" 2>&1)
          QUERY_EXIT_CODE=$?
          set -e

          if [ $QUERY_EXIT_CODE -ne 0 ]; then
            echo "âš ï¸  Query failed - may be missing data"
            echo "   R script will handle gracefully"
            echo "has_matches=unknown" >> "$GITHUB_OUTPUT"
            echo "match_count=0" >> "$GITHUB_OUTPUT"
            echo "athlete_count=0" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          # Parse results
          MATCH_COUNT=$(echo "$MATCH_RESULT" | tail -n1 | cut -d',' -f1)
          ATHLETE_COUNT=$(echo "$MATCH_RESULT" | tail -n1 | cut -d',' -f2)

          echo "ğŸ“Š Match Results:"
          echo "   Matching records: $MATCH_COUNT"
          echo "   Athletes with data: $ATHLETE_COUNT"
          echo ""

          if [ "$MATCH_COUNT" -gt 0 ]; then
            echo "âœ… Found matching data â†’ Training will proceed"
            echo "has_matches=true" >> "$GITHUB_OUTPUT"
          else
            echo "âš ï¸  No matching data â†’ Training will be skipped"
            echo "has_matches=false" >> "$GITHUB_OUTPUT"
          fi

          echo "match_count=$MATCH_COUNT" >> "$GITHUB_OUTPUT"
          echo "athlete_count=$ATHLETE_COUNT" >> "$GITHUB_OUTPUT"

    outputs:
      has_matches: ${{ steps.check_matches.outputs.has_matches }}
      match_count: ${{ steps.check_matches.outputs.match_count }}
      athlete_count: ${{ steps.check_matches.outputs.athlete_count }}
      sync_action: ${{ steps.sync_readiness.outputs.sync_action }}

  train-models:
    runs-on: ubuntu-latest
    needs: [check-readiness]
    if: |
      needs.check-readiness.outputs.has_matches == 'true' &&
      github.event.inputs.skip_training != 'true'

    steps:
      - uses: actions/checkout@v4

      # FIX 6: Authenticate to BOTH projects for R script
      - name: Auth to Readiness Project (for reading VALD data)
        uses: google-github-actions/auth@v2
        id: auth_readiness
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY_READINESS }}
          create_credentials_file: true
          cleanup_credentials: false

      - name: Auth to ML Project (for writes)
        uses: google-github-actions/auth@v2
        id: auth_ml
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY_ML }}
          create_credentials_file: true
          export_environment_variables: true

      - uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.GCP_PROJECT_ML }}

      # FIX 7: Export both credential paths to R script
      - name: Setup dual-project credentials for R
        run: |
          echo "GOOGLE_APPLICATION_CREDENTIALS_READINESS=${{ steps.auth_readiness.outputs.credentials_file_path }}" >> $GITHUB_ENV
          echo "GOOGLE_APPLICATION_CREDENTIALS=${{ steps.auth_ml.outputs.credentials_file_path }}" >> $GITHUB_ENV

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install scikit-learn==1.5.1 numpy==1.26.4

      - uses: r-lib/actions/setup-r@v2
        with:
          use-public-rspm: true

      - uses: r-lib/actions/setup-r-dependencies@v2
        with:
          cache-version: 1
          extra-packages: |
            any::bigrquery
            any::DBI
            any::dplyr
            any::tidyr
            any::data.table
            any::lubridate
            any::slider
            any::stringr
            any::gargle
            any::glue
            any::digest
            any::glmnet
            any::bnlearn
            any::reticulate

      - name: System libs
        run: |
          sudo apt-get update
          sudo apt-get install -y libcurl4-openssl-dev libssl-dev libxml2-dev jq

      - name: Verify BigQuery write access
        env:
          GCP_PROJECT_ML: ${{ env.GCP_PROJECT_ML }}
          BQ_DATASET:  ${{ env.BQ_DATASET }}
          BQ_LOCATION: ${{ env.BQ_LOCATION }}
        run: |
          set -euo pipefail
          echo "=== BigQuery Access Validation ==="
          echo "Checking access to dataset ${BQ_DATASET} in ML project ${GCP_PROJECT_ML}..."

          if ! bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" ls "${BQ_DATASET}" >/dev/null 2>&1; then
            echo ""
            echo "âŒ Cannot list dataset ${BQ_DATASET}"
            echo ""
            echo "Troubleshooting:"
            echo "  1. Service account needs BigQuery Data Editor permission"
            echo "  2. Or roles/bigquery.dataEditor on the dataset"
            echo "  3. Verify GCP_SA_KEY_ML has BigQuery write access"
            echo ""
            echo "Attempting to show more details:"
            bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" ls 2>&1 || true
            exit 1
          fi
          echo "âœ… BigQuery dataset accessible"

          # Verify we can query the dataset
          if ! bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" query --use_legacy_sql=false "SELECT 1" >/dev/null 2>&1; then
            echo "âŒ Cannot execute queries in BigQuery"
            exit 1
          fi
          echo "âœ… BigQuery write access verified"

      - name: Run model training
        env:
          GCP_PROJECT: ${{ env.GCP_PROJECT_ML }}
          GCP_PROJECT_READINESS: ${{ env.GCP_PROJECT_READINESS }}
          BQ_DATASET:  ${{ env.BQ_DATASET }}
          BQ_LOCATION: ${{ env.BQ_LOCATION }}
          TEAM_NAME:   ${{ env.TEAM_NAME }}

          WORKLOAD_TABLE:    ${{ env.WORKLOAD_TABLE }}
          READINESS_TABLE:   ${{ env.READINESS_TABLE }}
          ROSTER_TABLE:      ${{ env.ROSTER_TABLE }}
          PREDICTIONS_TABLE: ${{ env.PREDICTIONS_TABLE }}

          START_DATE:        ${{ github.event.inputs.start_date || '2025-03-01' }}
          END_DATE:          ${{ github.event.inputs.end_date   || '2026-12-31' }}
          TRAIN_FRACTION:    '0.80'
          MIN_TRAIN_OBS:     '3'

          HAS_MATCHES:       ${{ needs.check-readiness.outputs.has_matches }}
          MATCH_LOOKBACK_DAYS: '7'
          SKIP_TRAINING:     ${{ github.event.inputs.skip_training || 'false' }}
          SKIP_REGISTRY_STAGES: 'true'  # BigQuery free tier doesn't support DML (INSERT) queries
        run: Rscript ".github/scripts/readiness_models_cloud.R"

      - name: Show today's training summary (best-effort)
        if: success()
        env:
          GCP_PROJECT_ML: ${{ env.GCP_PROJECT_ML }}
          BQ_DATASET:  ${{ env.BQ_DATASET }}
          BQ_LOCATION: ${{ env.BQ_LOCATION }}
        run: |
          set -eo pipefail
          if ! command -v envsubst >/dev/null 2>&1; then
            sudo apt-get update && sudo apt-get install -y gettext-base
          fi

          # Check if model_training_summary table exists before querying
          TABLE_EXISTS=$(bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" \
            ls -n 1000 "${BQ_DATASET}" 2>/dev/null | grep -c "model_training_summary" || echo "0")

          echo ""
          echo "ğŸ“ˆ Latest Models:"

          if [ "$TABLE_EXISTS" -eq "0" ]; then
            echo "No training results yet (table not created)"
          else
            # Use multi-line string instead of heredoc for proper YAML syntax
            Q1='
            SELECT
              athlete_id,
              roster_name,
              model_type,
              ROUND(primary_rmse, 3) AS rmse,
              n_train,
              FORMAT_TIMESTAMP("%H:%M", trained_at) AS time
            FROM `${GCP_PROJECT_ML}.${BQ_DATASET}.model_training_summary`
            WHERE DATE(trained_at) = CURRENT_DATE()
            ORDER BY trained_at DESC
            LIMIT 10
            '
            Q1="$(envsubst '${GCP_PROJECT_ML} ${BQ_DATASET}' <<<"$Q1")"
            bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" query --use_legacy_sql=false --max_rows=10 "$Q1" 2>&1 || echo "No training results yet"
          fi

          echo ""
          echo "ğŸ“Š Today's Training Stats:"

          if [ "$TABLE_EXISTS" -eq "0" ]; then
            echo "No stats available (table not created)"
          else
            Q2='
            SELECT
              COUNT(DISTINCT athlete_id) AS athletes_trained,
              COUNT(*) AS total_models,
              ROUND(AVG(primary_rmse), 3) AS avg_rmse,
              FORMAT_TIMESTAMP("%H:%M", MAX(trained_at)) AS last_run
            FROM `${GCP_PROJECT_ML}.${BQ_DATASET}.model_training_summary`
            WHERE DATE(trained_at) = CURRENT_DATE()
            '
            Q2="$(envsubst '${GCP_PROJECT_ML} ${BQ_DATASET}' <<<"$Q2")"
            bq --location="$BQ_LOCATION" --project_id="$GCP_PROJECT_ML" query --use_legacy_sql=false "$Q2" 2>&1 || echo "No stats available"
          fi

      - name: Cleanup credentials
        if: always()
        run: |
          if [ -n "${GOOGLE_APPLICATION_CREDENTIALS_READINESS:-}" ] && [ -f "${GOOGLE_APPLICATION_CREDENTIALS_READINESS}" ]; then
            rm -f "${GOOGLE_APPLICATION_CREDENTIALS_READINESS}"
          fi

  summary:
    runs-on: ubuntu-latest
    needs: [ingest, check-readiness, train-models]
    if: always()
    steps:
      - name: Pipeline Summary
        env:
          HAS_MATCHES:  ${{ needs.check-readiness.outputs.has_matches }}
          MATCH_COUNT:  ${{ needs.check-readiness.outputs.match_count }}
          ATH_COUNT:    ${{ needs.check-readiness.outputs.athlete_count }}
        run: |
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "    WORKLOAD & MODEL PIPELINE SUMMARY"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo ""
          echo "ğŸ”¹ Workload Ingest:     ${{ needs.ingest.result }}"
          echo "ğŸ”¹ Readiness Check:     ${{ needs.check-readiness.result }}"
          echo "ğŸ”¹ Model Training:      ${{ needs.train-models.result }}"
          echo ""

          if [ "${{ needs.ingest.result }}" != "success" ]; then
            echo "âŒ INGEST FAILED - Check workload ingest logs"
            exit 1

          elif [ "${{ needs.check-readiness.result }}" == "skipped" ]; then
            echo "â­ï¸  PIPELINE SKIPPED - Ingest failed, no readiness check performed"
            exit 1

          elif [ "$HAS_MATCHES" != "true" ]; then
            echo "âœ… INGEST COMPLETE - Workload data saved"
            echo "â­ï¸  TRAINING SKIPPED - No matching readiness data found (OK)"
            exit 0

          elif [ "${{ needs.train-models.result }}" == "skipped" ]; then
            echo "âœ… INGEST COMPLETE - Workload data saved"
            echo "âœ… READINESS CHECK - Found $MATCH_COUNT matches across $ATH_COUNT athletes"
            echo "â­ï¸  TRAINING SKIPPED - Manual skip requested"
            exit 0

          elif [ "${{ needs.train-models.result }}" == "success" ]; then
            echo "âœ… PIPELINE COMPLETE - All jobs succeeded!"
            echo ""
            echo "ğŸ“Š Summary:"
            echo "   â€¢ Matches found: $MATCH_COUNT"
            echo "   â€¢ Athletes trained: $ATH_COUNT"
            exit 0

          elif [ "${{ needs.train-models.result }}" == "failure" ]; then
            echo "âœ… INGEST COMPLETE - Workload data saved"
            echo "âœ… READINESS CHECK - Found $MATCH_COUNT matches"
            echo "âŒ TRAINING FAILED - Check model training logs"
            exit 0

          else
            echo "âš ï¸  UNKNOWN STATUS - Check individual job logs"
            exit 1

          fi
